{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf32add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import strip_markdown\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510045b2",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "1. Filter comments that contains bot garbe massage. specifically filtering the markdown table format\n",
    "2. Remove markdown formating string\n",
    "3. Romove URL\n",
    "4. Remove returning line \"\\n\"\n",
    "5. Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_parquet(csv_file)\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "    file_name = file_name.split(\".pq\")[0]\n",
    "    if \"new\" in file_name:\n",
    "        df['subreddit'] = file_name.split(\"_\")[0]\n",
    "        df['topic'] = \"all\"\n",
    "    else:\n",
    "        df['subreddit'] = \"WSB\"\n",
    "        if \"bets\" in file_name:\n",
    "            df['topic'] = file_name.split(\"_\")[1]\n",
    "        else:\n",
    "            df['topic'] = file_name.split(\"_\")[0]\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c7e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.concat(dataframes).drop(['distinguished'],axis=1)\n",
    "sample = sample.rename(columns={'cbody':'Comment Body','title':'Post Title','selftext':'Post Selftxt'})\n",
    "sample = sample[~sample['Comment Body'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2e0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They faked it? Maybe they had my girlfriend help them as she is great at faking it.\n",
      "Fake it till you make it, right? ![img](emote|t5_2th52|4260)\n"
     ]
    }
   ],
   "source": [
    "for i in sample['Comment Body'].head(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f15ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs from a given text string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d802fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['Comment Body'] = sample['Comment Body'].apply(strip_markdown.strip_markdown)  # clean all markdown format\n",
    "sample['Comment Body'] = sample['Comment Body'].apply(remove_urls) # remove urls\n",
    "sample['Comment Body'] = sample['Comment Body'].str.strip('\\n')\n",
    "\n",
    "# 去除emoji\n",
    "sample['Post Title'] = sample['Post Title'].apply(remove_emoji)\n",
    "sample['Post Selftxt'] = sample['Post Selftxt'].apply(remove_emoji)\n",
    "sample['Comment Body'] = sample['Comment Body'].apply(remove_emoji)\n",
    "# sample[['Post Title','Post Selftxt','Comment Body']].to_excel('Demoji.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da8e28",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "1. Tokenize comments by LLM (Huggingface transformers)\n",
    "2. Use Loughran-McDonald Master Dictionary w/ Sentiment Word Lists (which specifically focus on financial wording) https://sraf.nd.edu/loughranmcdonald-master-dictionary/ to assign sentiment scores and extract positive/negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4e9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentiment2_updated import LM # updated pysentiment2 with the lastest version of LM dictionary\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# output = {'Positive cnt':[],'Negative cnt':[],'Polarity':[],'Subjectivity':[],'Positive words':[],'Negative words':[],'Tokenized':[]}\n",
    "lm = LM()\n",
    "def sentiment_analysis(text):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    score = lm.get_score(tokens)\n",
    "    return [score['Positive'],score['Negative'],score['Polarity'],score['Subjectivity'],score['Positive words'],score['Negative words'],score['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706e1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "output = sample['Comment Body'].apply(sentiment_analysis)\n",
    "output = pd.DataFrame(output)\n",
    "\n",
    "def get_item(series,index):\n",
    "    return series[index]\n",
    "\n",
    "sample['Positive cnt']=output['Comment Body'].apply(get_item, index=0)\n",
    "sample['Negative cnt'] = output['Comment Body'].apply(get_item, index=1)\n",
    "sample['Polarity'] = output['Comment Body'].apply(get_item, index=2)\n",
    "sample['Subjectivity'] = output['Comment Body'].apply(get_item, index=3)\n",
    "sample['Positive words'] = output['Comment Body'].apply(get_item, index=4)\n",
    "sample['Negative words'] = output['Comment Body'].apply(get_item, index=5)\n",
    "sample['Tokenized'] = output['Comment Body'].apply(get_item, index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e9410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[['id','Post Title','time','Positive cnt','Comment Body','cnum_replies','subreddit', 'topic',\\\n",
    "        'Negative cnt','Polarity','Subjectivity','Positive words','Negative words'\\\n",
    "        ,'Tokenized']].to_parquet('output/data_sample_3w_output.pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980fbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/stock_data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "stock_data = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['company'] = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    stock_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0257a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = pd.concat(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0c4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "    if file_name[:4]==\"bets\":\n",
    "        topic = file_name.strip(\".pq\").split(\"_\")[1].upper()\n",
    "        \n",
    "#     topic = file_name.strip(\".pq\").split(\"_\")[1]\n",
    "        df = pd.read_parquet(csv_file)\n",
    "#     df['subreddit'] = subreddit\n",
    "#     df['topic'] = topic\n",
    "        text_data= df['cbody'][:1000].str.cat()\n",
    "        f = open(\"output/\"+topic+\"_wordcloud.txt\", \"a\")\n",
    "        f.write(text_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331fccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('output/data_sample_3w_output.pq')\n",
    "df = df.rename(columns={'time':'Date'})\n",
    "df['Date'] = [x.strftime(\"%Y-%m-%d\") for x in df['Date']]\n",
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/stock_data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "stock_data = []\n",
    "company = []\n",
    "for csv_file in csv_files:\n",
    "    stock_df = pd.read_csv(csv_file)[['Date','Close']]\n",
    "    company_name = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    company.append(company_name)\n",
    "    stock_df = stock_df.rename(columns={\"Close\":\"StockPrice\"})\n",
    "#     stock_df['Date'] = [x.strftime(\"%Y-%m-%d\") for x in df['Date']]\n",
    "\n",
    "    stock_df['company'] = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    stock_data.append(stock_df)\n",
    "#     df = df.merge(stock_df,how=\"left\",on=\"Date\")\n",
    "stock_price = pd.concat(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "254e90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_parquet(\"output/stock_price.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9259cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini_sample\n",
      "bets_ai_search\n",
      "Google_sample\n",
      "OpenAI_new\n",
      "Microsoft_sample\n",
      "bets_nvda_search\n",
      "AMD_sample\n",
      "Sora_sample\n",
      "NVDA_Stock_new\n",
      "ChatGPT_sample\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "bet_company = []\n",
    "for csv_file in csv_files:\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "#     if file_name[:4]==\"bets\":\n",
    "    company_name = csv_file.split(\"/\")[-1].strip(\".pq\")\n",
    "    if \"new\" not in company_name:\n",
    "        bet_company.append(company_name.split(\"_\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64d82c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"output/data_sample_3w_output.pq\") \n",
    "words = df[df['subreddit']==\"WSB\"][['subreddit','topic','Positive words','Negative words']]\n",
    "topic_words_str = {\"topic\":[],\"pos_word\":[],'neg_word':[]}\n",
    "\n",
    "# topic_data = {\n",
    "# }\n",
    "# for i in subreddits:\n",
    "#     topic_data[i] = list(df[df['subreddit']==i]['Polarity'])\n",
    "def word_str(list):\n",
    "    return \" \".join(list)\n",
    "words['pos_str'] = words['Positive words'].apply(word_str)\n",
    "words['neg_str'] = words['Negative words'].apply(word_str)\n",
    "\n",
    "for i in set(words['topic']):\n",
    "    w = words[words['topic']==i]\n",
    "    pos = word_str(list(w['pos_str']))\n",
    "    pos = re.sub(r'\\s+', ' ', pos)\n",
    "    neg = word_str(list(w['neg_str']))\n",
    "    neg = re.sub(r'\\s+', ' ', neg)\n",
    "    topic_words_str['topic'].append(i)\n",
    "    topic_words_str['pos_word'].append(pos)\n",
    "    topic_words_str['neg_word'].append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61497994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topic_words_str).to_parquet(\"output/wsb_topic_words.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6efa164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>pos_word</th>\n",
       "      <th>neg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>leadership reward better great superior easie...</td>\n",
       "      <td>break bad bubble bad fail shut stop lose ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMD</td>\n",
       "      <td>enjoy profit profit reward better profit prof...</td>\n",
       "      <td>er loss contract bad lose correct ill er lose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>better great loyal better better great better...</td>\n",
       "      <td>ill bad stop shut fake bad argument slow argu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>profit better boom lead progress profit stron...</td>\n",
       "      <td>bubble bubble bubble bubble bubble bubble los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sora</td>\n",
       "      <td>great dream revolution lead superior better p...</td>\n",
       "      <td>worst lose bubble bubble fake bankrupt conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>great better profit better better revolution b...</td>\n",
       "      <td>fake fake lie fraud fake stop fake lose lose l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nvda</td>\n",
       "      <td>profit profit great profit profit great profi...</td>\n",
       "      <td>resign scandal drop drop lose contract questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google</td>\n",
       "      <td>better win profit win win great better streng...</td>\n",
       "      <td>content suffer stop content loss bad fire pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic                                           pos_word  \\\n",
       "0  Microsoft   leadership reward better great superior easie...   \n",
       "1        AMD   enjoy profit profit reward better profit prof...   \n",
       "2    ChatGPT   better great loyal better better great better...   \n",
       "3         ai   profit better boom lead progress profit stron...   \n",
       "4       Sora   great dream revolution lead superior better p...   \n",
       "5     Gemini  great better profit better better revolution b...   \n",
       "6       nvda   profit profit great profit profit great profi...   \n",
       "7     Google   better win profit win win great better streng...   \n",
       "\n",
       "                                            neg_word  \n",
       "0   break bad bubble bad fail shut stop lose ques...  \n",
       "1   er loss contract bad lose correct ill er lose...  \n",
       "2   ill bad stop shut fake bad argument slow argu...  \n",
       "3   bubble bubble bubble bubble bubble bubble los...  \n",
       "4   worst lose bubble bubble fake bankrupt conten...  \n",
       "5  fake fake lie fraud fake stop fake lose lose l...  \n",
       "6   resign scandal drop drop lose contract questi...  \n",
       "7   content suffer stop content loss bad fire pro...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(topic_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261854f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
