{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abf32add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import strip_markdown\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510045b2",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "1. Filter comments that contains bot garbe massage. specifically filtering the markdown table format\n",
    "2. Remove markdown formating string\n",
    "3. Romove URL\n",
    "4. Remove returning line \"\\n\"\n",
    "5. Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_parquet(csv_file)\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "    file_name = file_name.split(\".pq\")[0]\n",
    "    if \"new\" in file_name:\n",
    "        df['subreddit'] = file_name.split(\"_\")[0]\n",
    "        df['topic'] = \"all\"\n",
    "    else:\n",
    "        df['subreddit'] = \"WSB\"\n",
    "        if \"bets\" in file_name:\n",
    "            df['topic'] = file_name.split(\"_\")[1]\n",
    "        else:\n",
    "            df['topic'] = file_name.split(\"_\")[0]\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c7e058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>author</th>\n",
       "      <th>time</th>\n",
       "      <th>Post Selftxt</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_top_level_comments</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>...</th>\n",
       "      <th>clink_id_pid</th>\n",
       "      <th>ctime</th>\n",
       "      <th>Comment Body</th>\n",
       "      <th>cdistinguished</th>\n",
       "      <th>cscore</th>\n",
       "      <th>cnum_replies</th>\n",
       "      <th>csubreddit_id_t</th>\n",
       "      <th>csubreddit_id_sid</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18dkkml</td>\n",
       "      <td>Google's best Gemini demo was faked</td>\n",
       "      <td>xoxoxoxoxo</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>It does amaze me how Alphabet dropped the ball...</td>\n",
       "      <td>831</td>\n",
       "      <td>0.89</td>\n",
       "      <td>225</td>\n",
       "      <td>46</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>18dkkml</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>They faked it? Maybe they had my girlfriend he...</td>\n",
       "      <td>None</td>\n",
       "      <td>576.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18dkkml</td>\n",
       "      <td>Google's best Gemini demo was faked</td>\n",
       "      <td>xoxoxoxoxo</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>It does amaze me how Alphabet dropped the ball...</td>\n",
       "      <td>831</td>\n",
       "      <td>0.89</td>\n",
       "      <td>225</td>\n",
       "      <td>46</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>18dkkml</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>Fake it till you make it, right? ![img](emote|...</td>\n",
       "      <td>None</td>\n",
       "      <td>210.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18dkkml</td>\n",
       "      <td>Google's best Gemini demo was faked</td>\n",
       "      <td>xoxoxoxoxo</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>It does amaze me how Alphabet dropped the ball...</td>\n",
       "      <td>831</td>\n",
       "      <td>0.89</td>\n",
       "      <td>225</td>\n",
       "      <td>46</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>18dkkml</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>That is not quacktastic</td>\n",
       "      <td>None</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18dkkml</td>\n",
       "      <td>Google's best Gemini demo was faked</td>\n",
       "      <td>xoxoxoxoxo</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>It does amaze me how Alphabet dropped the ball...</td>\n",
       "      <td>831</td>\n",
       "      <td>0.89</td>\n",
       "      <td>225</td>\n",
       "      <td>46</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>18dkkml</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>Oof. To have to make shit up to look impressiv...</td>\n",
       "      <td>None</td>\n",
       "      <td>122.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18dkkml</td>\n",
       "      <td>Google's best Gemini demo was faked</td>\n",
       "      <td>xoxoxoxoxo</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>It does amaze me how Alphabet dropped the ball...</td>\n",
       "      <td>831</td>\n",
       "      <td>0.89</td>\n",
       "      <td>225</td>\n",
       "      <td>46</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>18dkkml</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>Sounds like fraud to me.  To manipulate invest...</td>\n",
       "      <td>None</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21410</th>\n",
       "      <td>zv32ac</td>\n",
       "      <td>Lumber prices back to normal levels</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td></td>\n",
       "      <td>11016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>659</td>\n",
       "      <td>188</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>zv32ac</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>We always said that about gas stations.  The s...</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>ChatGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21411</th>\n",
       "      <td>zv32ac</td>\n",
       "      <td>Lumber prices back to normal levels</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td></td>\n",
       "      <td>11016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>659</td>\n",
       "      <td>188</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>zv32ac</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>Go cut a tree down and make lumber yourself th...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>ChatGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>zv32ac</td>\n",
       "      <td>Lumber prices back to normal levels</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td></td>\n",
       "      <td>11016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>659</td>\n",
       "      <td>188</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>zv32ac</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>I would fucking love to get my hands on some e...</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>ChatGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>zv32ac</td>\n",
       "      <td>Lumber prices back to normal levels</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td></td>\n",
       "      <td>11016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>659</td>\n",
       "      <td>188</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>zv32ac</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>I fucking did. Stole a Chrismas tree… \\nI am M...</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>ChatGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>zv32ac</td>\n",
       "      <td>Lumber prices back to normal levels</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td></td>\n",
       "      <td>11016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>659</td>\n",
       "      <td>188</td>\n",
       "      <td>2th52</td>\n",
       "      <td>...</td>\n",
       "      <td>zv32ac</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>That's not what happens when people stop being...</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5</td>\n",
       "      <td>2th52</td>\n",
       "      <td>WSB</td>\n",
       "      <td>ChatGPT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275021 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                           Post Title      author       time  \\\n",
       "1      18dkkml  Google's best Gemini demo was faked  xoxoxoxoxo 2023-12-08   \n",
       "2      18dkkml  Google's best Gemini demo was faked  xoxoxoxoxo 2023-12-08   \n",
       "3      18dkkml  Google's best Gemini demo was faked  xoxoxoxoxo 2023-12-08   \n",
       "4      18dkkml  Google's best Gemini demo was faked  xoxoxoxoxo 2023-12-08   \n",
       "5      18dkkml  Google's best Gemini demo was faked  xoxoxoxoxo 2023-12-08   \n",
       "...        ...                                  ...         ...        ...   \n",
       "21410   zv32ac  Lumber prices back to normal levels        None 2022-12-26   \n",
       "21411   zv32ac  Lumber prices back to normal levels        None 2022-12-26   \n",
       "21412   zv32ac  Lumber prices back to normal levels        None 2022-12-26   \n",
       "21413   zv32ac  Lumber prices back to normal levels        None 2022-12-26   \n",
       "21414   zv32ac  Lumber prices back to normal levels        None 2022-12-26   \n",
       "\n",
       "                                            Post Selftxt  score  upvote_ratio  \\\n",
       "1      It does amaze me how Alphabet dropped the ball...    831          0.89   \n",
       "2      It does amaze me how Alphabet dropped the ball...    831          0.89   \n",
       "3      It does amaze me how Alphabet dropped the ball...    831          0.89   \n",
       "4      It does amaze me how Alphabet dropped the ball...    831          0.89   \n",
       "5      It does amaze me how Alphabet dropped the ball...    831          0.89   \n",
       "...                                                  ...    ...           ...   \n",
       "21410                                                     11016          0.98   \n",
       "21411                                                     11016          0.98   \n",
       "21412                                                     11016          0.98   \n",
       "21413                                                     11016          0.98   \n",
       "21414                                                     11016          0.98   \n",
       "\n",
       "       num_comments  num_top_level_comments subreddit_id  ... clink_id_pid  \\\n",
       "1               225                      46        2th52  ...      18dkkml   \n",
       "2               225                      46        2th52  ...      18dkkml   \n",
       "3               225                      46        2th52  ...      18dkkml   \n",
       "4               225                      46        2th52  ...      18dkkml   \n",
       "5               225                      46        2th52  ...      18dkkml   \n",
       "...             ...                     ...          ...  ...          ...   \n",
       "21410           659                     188        2th52  ...       zv32ac   \n",
       "21411           659                     188        2th52  ...       zv32ac   \n",
       "21412           659                     188        2th52  ...       zv32ac   \n",
       "21413           659                     188        2th52  ...       zv32ac   \n",
       "21414           659                     188        2th52  ...       zv32ac   \n",
       "\n",
       "           ctime                                       Comment Body  \\\n",
       "1     2023-12-08  They faked it? Maybe they had my girlfriend he...   \n",
       "2     2023-12-08  Fake it till you make it, right? ![img](emote|...   \n",
       "3     2023-12-08                            That is not quacktastic   \n",
       "4     2023-12-08  Oof. To have to make shit up to look impressiv...   \n",
       "5     2023-12-09  Sounds like fraud to me.  To manipulate invest...   \n",
       "...          ...                                                ...   \n",
       "21410 2022-12-26  We always said that about gas stations.  The s...   \n",
       "21411 2022-12-26  Go cut a tree down and make lumber yourself th...   \n",
       "21412 2022-12-26  I would fucking love to get my hands on some e...   \n",
       "21413 2022-12-26  I fucking did. Stole a Chrismas tree… \\nI am M...   \n",
       "21414 2022-12-26  That's not what happens when people stop being...   \n",
       "\n",
       "      cdistinguished cscore cnum_replies csubreddit_id_t csubreddit_id_sid  \\\n",
       "1               None  576.0         15.0              t5             2th52   \n",
       "2               None  210.0         16.0              t5             2th52   \n",
       "3               None   65.0          2.0              t5             2th52   \n",
       "4               None  122.0         23.0              t5             2th52   \n",
       "5               None    7.0          0.0              t5             2th52   \n",
       "...              ...    ...          ...             ...               ...   \n",
       "21410           None    4.0          0.0              t5             2th52   \n",
       "21411           None    1.0          2.0              t5             2th52   \n",
       "21412           None    2.0          0.0              t5             2th52   \n",
       "21413           None    6.0          0.0              t5             2th52   \n",
       "21414           None    3.0          0.0              t5             2th52   \n",
       "\n",
       "       subreddit    topic  \n",
       "1            WSB   Gemini  \n",
       "2            WSB   Gemini  \n",
       "3            WSB   Gemini  \n",
       "4            WSB   Gemini  \n",
       "5            WSB   Gemini  \n",
       "...          ...      ...  \n",
       "21410        WSB  ChatGPT  \n",
       "21411        WSB  ChatGPT  \n",
       "21412        WSB  ChatGPT  \n",
       "21413        WSB  ChatGPT  \n",
       "21414        WSB  ChatGPT  \n",
       "\n",
       "[275021 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.concat(dataframes).drop(['distinguished'],axis=1)\n",
    "sample = sample.rename(columns={'cbody':'Comment Body','title':'Post Title','selftext':'Post Selftxt'})\n",
    "sample = sample[~sample['Comment Body'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f15ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs from a given text string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d802fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['Comment Body'] = sample['Comment Body'].apply(strip_markdown.strip_markdown)  # clean all markdown format\n",
    "sample['Comment Body'] = sample['Comment Body'].apply(remove_urls) # remove urls\n",
    "sample['Comment Body'] = sample['Comment Body'].str.strip('\\n')\n",
    "\n",
    "# 去除emoji\n",
    "sample['Post Title'] = sample['Post Title'].apply(remove_emoji)\n",
    "sample['Post Selftxt'] = sample['Post Selftxt'].apply(remove_emoji)\n",
    "sample['Comment Body'] = sample['Comment Body'].apply(remove_emoji)\n",
    "# sample[['Post Title','Post Selftxt','Comment Body']].to_excel('Demoji.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da8e28",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "1. Tokenize comments by LLM (Huggingface transformers)\n",
    "2. Use Loughran-McDonald Master Dictionary w/ Sentiment Word Lists (which specifically focus on financial wording) https://sraf.nd.edu/loughranmcdonald-master-dictionary/ to assign sentiment scores and extract positive/negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4e9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentiment2_updated import LM # updated pysentiment2 with the lastest version of LM dictionary\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# output = {'Positive cnt':[],'Negative cnt':[],'Polarity':[],'Subjectivity':[],'Positive words':[],'Negative words':[],'Tokenized':[]}\n",
    "lm = LM()\n",
    "def sentiment_analysis(text):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    score = lm.get_score(tokens)\n",
    "    return [score['Positive'],score['Negative'],score['Polarity'],score['Subjectivity'],score['Positive words'],score['Negative words'],score['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706e1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "output = sample['Comment Body'].apply(sentiment_analysis)\n",
    "output = pd.DataFrame(output)\n",
    "\n",
    "def get_item(series,index):\n",
    "    return series[index]\n",
    "\n",
    "sample['Positive cnt']=output['Comment Body'].apply(get_item, index=0)\n",
    "sample['Negative cnt'] = output['Comment Body'].apply(get_item, index=1)\n",
    "sample['Polarity'] = output['Comment Body'].apply(get_item, index=2)\n",
    "sample['Subjectivity'] = output['Comment Body'].apply(get_item, index=3)\n",
    "sample['Positive words'] = output['Comment Body'].apply(get_item, index=4)\n",
    "sample['Negative words'] = output['Comment Body'].apply(get_item, index=5)\n",
    "sample['Tokenized'] = output['Comment Body'].apply(get_item, index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e9410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[['id','Post Title','time','Positive cnt','Comment Body','cnum_replies','subreddit', 'topic',\\\n",
    "        'Negative cnt','Polarity','Subjectivity','Positive words','Negative words'\\\n",
    "        ,'Tokenized']].to_parquet('output/data_sample_3w_output.pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980fbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/stock_data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "stock_data = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['company'] = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    stock_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0257a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = pd.concat(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0c4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "    if file_name[:4]==\"bets\":\n",
    "        topic = file_name.strip(\".pq\").split(\"_\")[1].upper()\n",
    "        \n",
    "#     topic = file_name.strip(\".pq\").split(\"_\")[1]\n",
    "        df = pd.read_parquet(csv_file)\n",
    "#     df['subreddit'] = subreddit\n",
    "#     df['topic'] = topic\n",
    "        text_data= df['cbody'][:1000].str.cat()\n",
    "        f = open(\"output/\"+topic+\"_wordcloud.txt\", \"a\")\n",
    "        f.write(text_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331fccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('output/data_sample_3w_output.pq')\n",
    "df = df.rename(columns={'time':'Date'})\n",
    "df['Date'] = [x.strftime(\"%Y-%m-%d\") for x in df['Date']]\n",
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/stock_data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "stock_data = []\n",
    "company = []\n",
    "for csv_file in csv_files:\n",
    "    stock_df = pd.read_csv(csv_file)[['Date','Close']]\n",
    "    company_name = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    company.append(company_name)\n",
    "    stock_df = stock_df.rename(columns={\"Close\":\"StockPrice\"})\n",
    "#     stock_df['Date'] = [x.strftime(\"%Y-%m-%d\") for x in df['Date']]\n",
    "\n",
    "    stock_df['company'] = csv_file.split(\"/\")[-1].strip(\".csv\")\n",
    "    stock_data.append(stock_df)\n",
    "#     df = df.merge(stock_df,how=\"left\",on=\"Date\")\n",
    "stock_price = pd.concat(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "254e90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_parquet(\"output/stock_price.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9259cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini_sample\n",
      "bets_ai_search\n",
      "Google_sample\n",
      "OpenAI_new\n",
      "Microsoft_sample\n",
      "bets_nvda_search\n",
      "AMD_sample\n",
      "Sora_sample\n",
      "NVDA_Stock_new\n",
      "ChatGPT_sample\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/Users/chelseayeung/Documents/MFIN7036-Group-Project/Analysis/data\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.pq\"))\n",
    "\n",
    "bet_company = []\n",
    "for csv_file in csv_files:\n",
    "    file_name = csv_file.split(\"/\")[-1]\n",
    "#     if file_name[:4]==\"bets\":\n",
    "    company_name = csv_file.split(\"/\")[-1].strip(\".pq\")\n",
    "    if \"new\" not in company_name:\n",
    "        bet_company.append(company_name.split(\"_\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64d82c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"output/data_sample_3w_output.pq\") \n",
    "words = df[df['subreddit']==\"WSB\"][['subreddit','topic','Positive words','Negative words']]\n",
    "topic_words_str = {\"topic\":[],\"pos_word\":[],'neg_word':[]}\n",
    "\n",
    "# topic_data = {\n",
    "# }\n",
    "# for i in subreddits:\n",
    "#     topic_data[i] = list(df[df['subreddit']==i]['Polarity'])\n",
    "def word_str(list):\n",
    "    return \" \".join(list)\n",
    "words['pos_str'] = words['Positive words'].apply(word_str)\n",
    "words['neg_str'] = words['Negative words'].apply(word_str)\n",
    "\n",
    "for i in set(words['topic']):\n",
    "    w = words[words['topic']==i]\n",
    "    pos = word_str(list(w['pos_str']))\n",
    "    pos = re.sub(r'\\s+', ' ', pos)\n",
    "    neg = word_str(list(w['neg_str']))\n",
    "    neg = re.sub(r'\\s+', ' ', neg)\n",
    "    topic_words_str['topic'].append(i)\n",
    "    topic_words_str['pos_word'].append(pos)\n",
    "    topic_words_str['neg_word'].append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61497994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topic_words_str).to_parquet(\"output/wsb_topic_words.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6efa164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>pos_word</th>\n",
       "      <th>neg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>leadership reward better great superior easie...</td>\n",
       "      <td>break bad bubble bad fail shut stop lose ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMD</td>\n",
       "      <td>enjoy profit profit reward better profit prof...</td>\n",
       "      <td>er loss contract bad lose correct ill er lose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>better great loyal better better great better...</td>\n",
       "      <td>ill bad stop shut fake bad argument slow argu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>profit better boom lead progress profit stron...</td>\n",
       "      <td>bubble bubble bubble bubble bubble bubble los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sora</td>\n",
       "      <td>great dream revolution lead superior better p...</td>\n",
       "      <td>worst lose bubble bubble fake bankrupt conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>great better profit better better revolution b...</td>\n",
       "      <td>fake fake lie fraud fake stop fake lose lose l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nvda</td>\n",
       "      <td>profit profit great profit profit great profi...</td>\n",
       "      <td>resign scandal drop drop lose contract questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google</td>\n",
       "      <td>better win profit win win great better streng...</td>\n",
       "      <td>content suffer stop content loss bad fire pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic                                           pos_word  \\\n",
       "0  Microsoft   leadership reward better great superior easie...   \n",
       "1        AMD   enjoy profit profit reward better profit prof...   \n",
       "2    ChatGPT   better great loyal better better great better...   \n",
       "3         ai   profit better boom lead progress profit stron...   \n",
       "4       Sora   great dream revolution lead superior better p...   \n",
       "5     Gemini  great better profit better better revolution b...   \n",
       "6       nvda   profit profit great profit profit great profi...   \n",
       "7     Google   better win profit win win great better streng...   \n",
       "\n",
       "                                            neg_word  \n",
       "0   break bad bubble bad fail shut stop lose ques...  \n",
       "1   er loss contract bad lose correct ill er lose...  \n",
       "2   ill bad stop shut fake bad argument slow argu...  \n",
       "3   bubble bubble bubble bubble bubble bubble los...  \n",
       "4   worst lose bubble bubble fake bankrupt conten...  \n",
       "5  fake fake lie fraud fake stop fake lose lose l...  \n",
       "6   resign scandal drop drop lose contract questi...  \n",
       "7   content suffer stop content loss bad fire pro...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(topic_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261854f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
